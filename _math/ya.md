---
title:  "Ya"
layout: default

---

# optimization

<http://www.machinelearning.ru/wiki/index.php?title=момо>

One-dimentional optimization: 
<https://en.wikipedia.org/wiki/Golden-section_search>
<https://en.wikipedia.org/wiki/Successive_parabolic_interpolation>, <http://stu.sernam.ru/book_dig_m.php?id=93>
<https://en.wikipedia.org/wiki/Brent%27s_method>

Gradient, gradient descent.
<https://en.wikipedia.org/wiki/Gradient_method>
<http://wiki.fast.ai/index.php/Gradient_Descent>

<https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization>

<https://en.wikipedia.org/wiki/Quasi-Newton_method>, <https://en.wikipedia.org/wiki/Limited-memory_BFGS>
<https://stats.stackexchange.com/questions/284712/how-does-the-l-bfgs-work>

Convex functions and sets, their properties: <http://www.machinelearning.ru/wiki/images/archive/8/87/20170921192058%21MOMO17_Seminar4.pdf>

<https://en.wikipedia.org/wiki/Constrained_optimization>, Karush–Kuhn–Tucker conditions, <https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions>

<https://en.wikipedia.org/wiki/Linear_programming>
<https://en.wikipedia.org/wiki/Quadratic_programming>
<https://en.wikipedia.org/wiki/Quadratically_constrained_quadratic_program>

<https://en.wikipedia.org/wiki/Stochastic_gradient_descent>, modifications: momentum, RMSProp, Adam

